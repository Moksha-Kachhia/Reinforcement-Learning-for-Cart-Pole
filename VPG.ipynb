{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba2d32b6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Vanilla Policy Gradient Implementation for Cart Pole V1 (**Shadowing sabrinahirani 's Jupyter Book**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6173d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import *\n",
    "\n",
    "from torch.distributions import Categorical #unlike Normal, it has discrete probability distribution--best for action--rather than regression tasks/uncertainties\n",
    "\n",
    "import gym # initiallizing env & agent\n",
    "\n",
    "environment = gym.make(\"CartPole-v1\")\n",
    "\n",
    "obs_space = environment.observation_space.shape[0]\n",
    "action_space = environment.action_space.n\n",
    "\n",
    "print(f\"=== CartPole-v1 Environment ===\")\n",
    "print(f\" Observation Space:\", obs_space)\n",
    "print(f\" Action Space:\", action_space)\n",
    "\n",
    "agent = Agent(obs_space, action_space)\n",
    "\n",
    "n_average = 50 # episodes used for avg loss calc. \n",
    "threshold_loss = -15 #allowable limit of loss during any traj. \n",
    "\n",
    "history = [] #to keep track of losses\n",
    "n_trajectories = 800 #maximum trajectories \n",
    "\n",
    "for i in range(n_trajectories + 1):\n",
    "\n",
    "    state = environment.reset() # starting new trajectory\n",
    "\n",
    "    rewards = [] # record sequence of rewards for policy update\n",
    "    log_probs = [] # record sequence of log_probs for policy update\n",
    "    entropies = [] # record sequence of entropies for entropy regularization\n",
    "\n",
    "    T = 1000\n",
    "    for t in range(1, T):\n",
    "        \n",
    "        # select action\n",
    "        action_selected, log_prob, entropy = agent.action(state)\n",
    "\n",
    "        # get feedback from environment based on the action selected\n",
    "        state, reward, done, _ = environment.step(action_selected)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        entropies.append(entropy)\n",
    "\n",
    "        # terminate if needed (ie. max trajectories reached || threshold crossed)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # update policy with latest trajectory\n",
    "    loss = agent.learn(rewards, log_probs, entropies)\n",
    "    reward = rewards[-1]\n",
    "    history.append(loss)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        avg_loss = np.mean(history[max(0, i-n_average):(i+1)])\n",
    "        print(f\"Episode: {i:4} | Average Loss: {avg_loss:3.4f} | Current Loss: {loss:3.4f} | Current Reward {reward}\")\n",
    "\n",
    "        if avg_loss <= threshold_loss:\n",
    "\n",
    "            print(f'Reached loss threshold in {i} episodes')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa66fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module): #making the policy \n",
    "    def __init__(self, n_obs_dim: int, n_action_dim: int, n_hidden_dim = 128): \n",
    "        super().__init__() #accessing constructor of Vanilla aka nn.module to make the neural network layers\n",
    "        self.fc1 = nn.Linear(n_obs_dim, n_hidden_dim) #fully connected linear layer takes inputs from observation dims and outputs the hidden dim values \n",
    "        self.fc2 = nn.Linear(n_hidden_dim, n_action_dim) # FCLL that takes inputs from the hidden dim inputs and outputs action dim values\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: \n",
    "        \"\"\"sources: \n",
    "        https://medium.com/aimonks/a-comprehensive-guide-to-activation-functions-in-deep-learning-ff794f87c184\n",
    "        https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6.4-choosing-activation-functions/\"\"\"\n",
    "        x = F.relu(self.fc1(x)) #function for the hidden dimension\n",
    "        x = F.softmax(self.fc2(x), dim=1)  #function for output layer (prefered for discrete action spaces)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aa296d",
   "metadata": {},
   "source": [
    "Source: https://spinningup.openai.com/en/latest/algorithms/vpg.html#key-equations\n",
    "\n",
    "The policy's parameters are updated to maximize expected rewards by estimating how sensitive the policy's performance is to changes in its parameters. This update uses the concept of the advantage function, which helps determine whether a particular action was better or worse than expected, guiding the agent to improve its strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb939de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, obs_space: int, action_space: int):\n",
    "        self.alpha = 1e-3 # learning rate of 0.001 to ensure reasonable optimization size\n",
    "        self.gamma = 0.99 # long-term reward focus \n",
    "        self.eps =  np.finfo(np.float32).eps.item() # to avoid arithmatic problems \n",
    "        self.gradient_clipping = 0.5 # for gradient regulation \n",
    "        self.entropy_regularizationo = 0.5 # balance between exploration and exploitation\n",
    "        \n",
    "        self.policy = Network(obs_space, action_space)\n",
    "        self.optimizer = torch.optim.AdamW(self.policy.parameters(), lr=self.alpha) # optimizing policy network \n",
    "        \n",
    "    def action(self, state: np.ndarray) -> Tuple[int, torch.Tensor]:\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0) # converts the input state from a NumPy array to a PyTorch tensor\n",
    "        m = Categorical(self.policy(statestate)) # creating categorial distribution \n",
    "        action_selected = m.sample() #selecting the action based on the distribution \n",
    "        return action_selected.item(), m.log_prob(action), m.entropy() # the selected action, log prob. of the action and exploritative v.s exploitative behaviour val\n",
    "    \n",
    "    def learn(self, rewards: List[float], log_probs: List[torch.Tensor], entropies: List[torch.Tensor]) -> float:\n",
    "        ret = 0\n",
    "        returns = []\n",
    "        for r in rewards[::-1]: \n",
    "            ret = r + self.gamma * ret\n",
    "            returns.insert(0, ret)\n",
    "        \n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + self.eps)  \n",
    "\n",
    "        loss = []\n",
    "        for log_prob, ret, entropy in zip(log_probs, returns, entropies):\n",
    "            loss.append((-log_prob * ret) - (self.entropy_regularization * entropy)) \n",
    "        loss = torch.cat(loss).sum()\n",
    "\n",
    "        #gradient decent : backpropagation and optimizer step\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.policy.parameters(), self.gradient_clipping) \n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65255d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiallizing env & agent\n",
    "\n",
    "environment = gym.make(\"CartPole-v1\")\n",
    "\n",
    "obs_space = environment.observation_space.shape[0]\n",
    "action_space = environment.action_space.n\n",
    "\n",
    "print(f\"=== CartPole-v1 Environment ===\")\n",
    "print(f\" Observation Space:\", obs_space)\n",
    "print(f\" Action Space:\", action_space)\n",
    "\n",
    "agent = Agent(obs_space, action_space)\n",
    "\n",
    "n_average = 50 # episodes used for avg loss calc. \n",
    "threshold_loss = -15 #allowable limit of loss during any traj. \n",
    "\n",
    "history = [] #to keep track of losses\n",
    "n_trajectories = 800 #maximum trajectories \n",
    "\n",
    "for i in range(n_trajectories + 1):\n",
    "\n",
    "    state = environment.reset() # starting new trajectory\n",
    "\n",
    "    rewards = [] # record sequence of rewards for policy update\n",
    "    log_probs = [] # record sequence of log_probs for policy update\n",
    "    entropies = [] # record sequence of entropies for entropy regularization\n",
    "\n",
    "    T = 1000\n",
    "    for t in range(1, T):\n",
    "        \n",
    "        # select action\n",
    "        action_selected, log_prob, entropy = agent.action(state)\n",
    "\n",
    "        # get feedback from environment based on the action selected\n",
    "        state, reward, done, _ = environment.step(action_selected)\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        entropies.append(entropy)\n",
    "\n",
    "        # terminate if needed (ie. max trajectories reached || threshold crossed)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # update policy with latest trajectory\n",
    "    loss = agent.learn(rewards, log_probs, entropies)\n",
    "    reward = rewards[-1]\n",
    "    history.append(loss)\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        avg_loss = np.mean(history[max(0, i-n_average):(i+1)])\n",
    "        print(f\"Episode: {i:4} | Average Loss: {avg_loss:3.4f} | Current Loss: {loss:3.4f} | Current Reward {reward}\")\n",
    "\n",
    "        if avg_loss <= threshold_loss:\n",
    "\n",
    "            print(f'Reached loss threshold in {i} episodes')\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
